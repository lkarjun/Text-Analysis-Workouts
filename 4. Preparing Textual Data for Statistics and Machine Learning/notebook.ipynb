{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "290d00f3",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c6238f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_PATH = Path(\"../DATASETS/4\")\n",
    "\n",
    "post_file = BASE_PATH/'rspct_autos.tsv.gz'\n",
    "subred_file = BASE_PATH/'subreddit_info.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "59ebca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df = pd.read_csv(post_file, sep = '\\t')\n",
    "subred_df = pd.read_csv(subred_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "446c566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subred_df.set_index(['subreddit'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e8863dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = post_df.join(subred_df, on = 'subreddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6ad158b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>category_3</th>\n",
       "      <th>in_data</th>\n",
       "      <th>reason_for_exclusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8f73s7</td>\n",
       "      <td>Harley</td>\n",
       "      <td>No Club Colors</td>\n",
       "      <td>Funny story. I went to college in Las Vegas. T...</td>\n",
       "      <td>autos</td>\n",
       "      <td>harley davidson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5s0q8r</td>\n",
       "      <td>Mustang</td>\n",
       "      <td>Roush vs Shleby GT500</td>\n",
       "      <td>I am trying to determine which is faster, and ...</td>\n",
       "      <td>autos</td>\n",
       "      <td>ford</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5z3405</td>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>2001 Golf Wagon looking for some insight</td>\n",
       "      <td>Hello! &lt;lb&gt;&lt;lb&gt;Trying to find some information...</td>\n",
       "      <td>autos</td>\n",
       "      <td>VW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7df18v</td>\n",
       "      <td>Lexus</td>\n",
       "      <td>IS 250 Coolant Flush/Change</td>\n",
       "      <td>https://www.cars.com/articles/how-often-should...</td>\n",
       "      <td>autos</td>\n",
       "      <td>lexus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5tpve8</td>\n",
       "      <td>volt</td>\n",
       "      <td>Gen1 mpg w/ dead battery?</td>\n",
       "      <td>Hi, new to this subreddit.  I'm considering bu...</td>\n",
       "      <td>autos</td>\n",
       "      <td>chevrolet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id   subreddit                                     title  \\\n",
       "0  8f73s7      Harley                            No Club Colors   \n",
       "1  5s0q8r     Mustang                     Roush vs Shleby GT500   \n",
       "2  5z3405  Volkswagen  2001 Golf Wagon looking for some insight   \n",
       "3  7df18v       Lexus               IS 250 Coolant Flush/Change   \n",
       "4  5tpve8        volt                 Gen1 mpg w/ dead battery?   \n",
       "\n",
       "                                            selftext category_1  \\\n",
       "0  Funny story. I went to college in Las Vegas. T...      autos   \n",
       "1  I am trying to determine which is faster, and ...      autos   \n",
       "2  Hello! <lb><lb>Trying to find some information...      autos   \n",
       "3  https://www.cars.com/articles/how-often-should...      autos   \n",
       "4  Hi, new to this subreddit.  I'm considering bu...      autos   \n",
       "\n",
       "        category_2 category_3  in_data reason_for_exclusion  \n",
       "0  harley davidson        NaN     True                  NaN  \n",
       "1             ford        NaN     True                  NaN  \n",
       "2               VW        NaN     True                  NaN  \n",
       "3            lexus        NaN     True                  NaN  \n",
       "4        chevrolet        NaN     True                  NaN  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85ade32",
   "metadata": {},
   "source": [
    "#### Standardizing Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "87d41bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'subreddit', 'title', 'selftext', 'category_1', 'category_2',\n",
      "       'category_3', 'in_data', 'reason_for_exclusion'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "88a1372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping = {\n",
    " 'id': 'id',\n",
    " 'subreddit': 'subreddit',\n",
    " 'title': 'title',\n",
    " 'selftext': 'text',\n",
    " 'category_1': 'category',\n",
    " 'category_2': 'subcategory',\n",
    " 'category_3': None, # no data\n",
    " 'in_data': None, # not needed\n",
    " 'reason_for_exclusion': None # not needed\n",
    "}\n",
    "# define remaining columns\n",
    "columns = [c for c in column_mapping.keys() if column_mapping[c] != None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a499637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[columns].rename(columns = column_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "364ec19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['category'] == 'autos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b0305537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>12295</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>76ff08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <td>Volkswagen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>Advice about 2006 Passat 2.0T?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>Hi everyone, I’m looking for a winter car and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <td>autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subcategory</th>\n",
       "      <td>VW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         12295\n",
       "id                                                      76ff08\n",
       "subreddit                                           Volkswagen\n",
       "title                           Advice about 2006 Passat 2.0T?\n",
       "text         Hi everyone, I’m looking for a winter car and ...\n",
       "category                                                 autos\n",
       "subcategory                                                 VW"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b1be67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b761c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "769881ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"reddit-selfposts.db\"\n",
    "con = sqlite3.connect(db_name)\n",
    "df.to_sql(\"posts\", con, index = False, if_exists = 'replace')\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "759dc329",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(db_name)\n",
    "df = pd.read_sql(\"select * from posts\", con)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f601d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84f79481",
   "metadata": {},
   "source": [
    "## Cleaning Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2c91af",
   "metadata": {},
   "source": [
    "### Identify Noise with Regular Experssions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "76a394fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "16ec17d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_SUSPICIOUS = re.compile(r'[&#<>{}\\[\\]\\\\]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "15666ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impurity(text, min_len = 0):\n",
    "    '''returns the share of suspicious characters in a text'''\n",
    "    len_text = len(text)\n",
    "    if text == None or len_text < min_len:\n",
    "        return 0\n",
    "    return len(RE_SUSPICIOUS.findall(text)) / len_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9dac3e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09009009009009009"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "After viewing the [PINKIEPOOL Trailer](https://www.youtu.be/watch?v=ieHRoHUg)\n",
    "it got me thinking about the best match ups.\n",
    "<lb>Here's my take:<lb><lb>[](/sp)[](/ppseesyou) Deadpool<lb>[](/sp)[](/ajsly)\n",
    "Captain America<lb>\"\"\"\n",
    "\n",
    "impurity(text, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1a55e127",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['impurity'] = df['text'].apply(impurity, min_len = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "363ccff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>impurity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19682</th>\n",
       "      <td>Looking at buying a 335i with 39k miles and 11...</td>\n",
       "      <td>0.214716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12357</th>\n",
       "      <td>I'm looking to lease an a4 premium plus automa...</td>\n",
       "      <td>0.165099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2730</th>\n",
       "      <td>Breakdown below:&lt;lb&gt;&lt;lb&gt;Elantra GT&lt;lb&gt;&lt;lb&gt;2.0L...</td>\n",
       "      <td>0.139130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12754</th>\n",
       "      <td>Bulbs Needed:&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;**194 LED BULB x8**&lt;l...</td>\n",
       "      <td>0.132411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10726</th>\n",
       "      <td>I currently have a deposit on a 2013 335is (CP...</td>\n",
       "      <td>0.129317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  impurity\n",
       "19682  Looking at buying a 335i with 39k miles and 11...  0.214716\n",
       "12357  I'm looking to lease an a4 premium plus automa...  0.165099\n",
       "2730   Breakdown below:<lb><lb>Elantra GT<lb><lb>2.0L...  0.139130\n",
       "12754  Bulbs Needed:<lb><lb><lb>**194 LED BULB x8**<l...  0.132411\n",
       "10726  I currently have a deposit on a 2013 335is (CP...  0.129317"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['text', 'impurity']].sort_values(by = 'impurity', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a3f8c0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp import count_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4f8f54be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;lb&gt;</th>\n",
       "      <td>100729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;tab&gt;</th>\n",
       "      <td>642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         freq\n",
       "token        \n",
       "<lb>   100729\n",
       "<tab>     642"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_words(df, \n",
    "            column='text', \n",
    "            preprocess=lambda doc: re.findall(r'<[\\w]*>', doc)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a1497b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c95baa11",
   "metadata": {},
   "source": [
    "### Removing Noise with Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "27cd7b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "de6359ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = html.unescape(text)\n",
    "    # tags like <tab>\n",
    "    text = re.sub(r'<[^<>]*>', ' ', text)\n",
    "    # markdown URLs like [Some text](https://....)\n",
    "    text = re.sub(r'\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)', r'\\1', text)\n",
    "    # text or code in brackets like [0]\n",
    "    text = re.sub(r'\\[[^\\[\\]]*\\]', ' ', text)\n",
    "    # standalone sequences of specials, matches &# but not #cool\n",
    "    text = re.sub(r'(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', text)\n",
    "    # standalone sequences of hyphens like --- or ==\n",
    "    text = re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)', ' ', text)\n",
    "    # sequences of white spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "63b1df89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After viewing the PINKIEPOOL Trailer it got me thinking about the best match ups. Here's my take: Deadpool Captain America\n",
      "Impurity:  0.0\n"
     ]
    }
   ],
   "source": [
    "clean_text = clean(text)\n",
    "print(clean_text)\n",
    "print(\"Impurity: \", impurity(clean_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fa55f85c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>impurity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14058</th>\n",
       "      <td>Mustang 2018, 2019, or 2020? Must Haves!! 1. H...</td>\n",
       "      <td>0.030864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18934</th>\n",
       "      <td>At the dealership, they offered an option for ...</td>\n",
       "      <td>0.026455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16505</th>\n",
       "      <td>I am looking at four Caymans, all are in a sim...</td>\n",
       "      <td>0.024631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              clean_text  impurity\n",
       "14058  Mustang 2018, 2019, or 2020? Must Haves!! 1. H...  0.030864\n",
       "18934  At the dealership, they offered an option for ...  0.026455\n",
       "16505  I am looking at four Caymans, all are in a sim...  0.024631"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['text'].map(clean)\n",
    "df['impurity'] = df['clean_text'].apply(impurity, min_len = 20)\n",
    "\n",
    "df[['clean_text', 'impurity']].sort_values(by = 'impurity', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d23ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b17cadd6",
   "metadata": {},
   "source": [
    "### Character Normalization with textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a5eb80fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The café “Saint-Raphaël” is loca-\\nted on Côte dʼAzur.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac866fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "da68ea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textacy import preprocessing as tprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "602bb269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    text = tprep.normalize.hyphenated_words(text)\n",
    "    text = tprep.normalize.quotation_marks(text)\n",
    "    text = tprep.normalize.unicode(text)\n",
    "    text = tprep.remove.accents(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "dde11424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cafe \"Saint-Raphael\" is located on Cote d'Azur.\n"
     ]
    }
   ],
   "source": [
    "print(normalize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04998646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e81a322c",
   "metadata": {},
   "source": [
    "### Pattern-Based Data Masking with textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b3b7f00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textacy.preprocessing.resources import RE_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "18042a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>www.getlowered.com</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://www.ecolamautomotive.com/#!2/kv7fq</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.reddit.com/r/Jeep/comments/4ux232/just_ordered_an_android_head_unit_joying_jeep/</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://imgur.com/gallery/XkRsw</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    freq\n",
       "token                                                   \n",
       "www.getlowered.com                                     3\n",
       "http://www.ecolamautomotive.com/#!2/kv7fq              2\n",
       "https://www.reddit.com/r/Jeep/comments/4ux232/j...     2\n",
       "http://imgur.com/gallery/XkRsw                         2"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_words(df, column='clean_text', preprocess=RE_URL.findall).head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "36e0da6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out _URL_\n"
     ]
    }
   ],
   "source": [
    "from textacy.preprocessing.replace import urls\n",
    "text = \"Check out https://spacy.io/usage/spacy-101\"\n",
    "# using default substitution _URL_\n",
    "print(urls(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e3ab0c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df['clean_text'].map(urls)\n",
    "df['clean_text'] = df['clean_text'].map(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0c983c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename({'text': 'raw_text', 'clean_text': 'text'}, inplace = True)\n",
    "df.drop(columns = ['impurity'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "eb318ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(db_name)\n",
    "df.to_sql('post_cleaned', con, index = False, if_exists='replace')\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb7550d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36f50a8b",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "123cd56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "2019-08-10 23:32: @pete/@louis - I don't have a well-designed\n",
    "solution for today's problem. The code of module AC68 should be -1.\n",
    "Have to think a bit... #goodnight ;-) ᥌᥏\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "45fab2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019|08|10|23|32|pete|louis|don|have|well|designed|solution|for|today|problem|The|code|of|module|AC68|should|be|Have|to|think|bit|goodnight|᥌᥏\n"
     ]
    }
   ],
   "source": [
    "tokens = re.findall(r'\\w\\w+', text)\n",
    "print(*tokens, sep='|')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e6a6196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_TOKEN = re.compile(r\"\"\"\n",
    " ( [#]?[@\\w'’\\.\\-\\:]*\\w # words, hashtags and email addresses\n",
    " | [:;<]\\-?[\\)\\(3] # coarse pattern for basic text emojis\n",
    " | [\\U0001F100-\\U0001FFFF] # coarse code range for unicode emojis\n",
    " )\n",
    " \"\"\", re.VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b2bb6675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-10|23:32|@pete|@louis|I|don't|have|a|well-designed|solution|for|today's|problem|The|code|of|module|AC68|should|be|-1|Have|to|think|a|bit|#goodnight|;-)|᥌᥏\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text): \n",
    "    return RE_TOKEN.findall(text)\n",
    "\n",
    "tokens = tokenize(text)\n",
    "print(*tokens, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab14f01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2982d385",
   "metadata": {},
   "source": [
    "### Tokenizatino with NTLK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e3142623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "8caa16ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = nltk.tokenize.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d8f25438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-10|23:32|:|@|pete/|@|louis|-|I|do|n't|have|a|well-designed|solution|for|today|'s|problem|.|The|code|of|module|AC68|should|be|-1|.|Have|to|think|a|bit|...|#|goodnight|;|-|)|᥌᥏\n"
     ]
    }
   ],
   "source": [
    "print(*token, sep = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e7169e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bdbe3bd",
   "metadata": {},
   "source": [
    "## Linguistic Processing with SpaCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f2110c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "5fe89a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "de17922b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x21d2daad0a0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x21d2daad4c0>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x21d2d7489e0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x21d2db16080>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x21d2db33f80>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x21d2d748200>)]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "9fbcb969",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f726312f",
   "metadata": {},
   "source": [
    "### Processing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b2a518d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "9d0c3143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My|best|friend|Ryan|Peters|likes|fancy|adventure|games|.\n"
     ]
    }
   ],
   "source": [
    "print(*doc, sep = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "a9c206db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_nlp(doc, include_punct=False):\n",
    "    \"\"\"Generate data frame for visualization of spaCy tokens.\"\"\"\n",
    "    rows = []\n",
    "    for i, t in enumerate(doc):\n",
    "        if not t.is_punct or include_punct:\n",
    "            row = {'token': i, 'text': t.text, 'lemma_': t.lemma_,\n",
    "                 'is_stop': t.is_stop, 'is_alpha': t.is_alpha,\n",
    "                 'pos_': t.pos_, 'dep_': t.dep_,\n",
    "                 'ent_type_': t.ent_type_, 'ent_iob_': t.ent_iob_}\n",
    "            rows.append(row)\n",
    "    df = pd.DataFrame(rows).set_index('token')\n",
    "    df.index.name = None\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "dfd5f978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma_</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>is_alpha</th>\n",
       "      <th>pos_</th>\n",
       "      <th>dep_</th>\n",
       "      <th>ent_type_</th>\n",
       "      <th>ent_iob_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My</td>\n",
       "      <td>my</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PRON</td>\n",
       "      <td>poss</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>best</td>\n",
       "      <td>good</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>amod</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>friend</td>\n",
       "      <td>friend</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>nsubj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ryan</td>\n",
       "      <td>Ryan</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>compound</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Peters</td>\n",
       "      <td>Peters</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>appos</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>likes</td>\n",
       "      <td>like</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ROOT</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fancy</td>\n",
       "      <td>fancy</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>amod</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>adventure</td>\n",
       "      <td>adventure</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>compound</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>games</td>\n",
       "      <td>game</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>dobj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text     lemma_  is_stop  is_alpha   pos_      dep_ ent_type_ ent_iob_\n",
       "0         My         my     True      True   PRON      poss                  O\n",
       "1       best       good    False      True    ADJ      amod                  O\n",
       "2     friend     friend    False      True   NOUN     nsubj                  O\n",
       "3       Ryan       Ryan    False      True  PROPN  compound    PERSON        B\n",
       "4     Peters     Peters    False      True  PROPN     appos    PERSON        I\n",
       "5      likes       like    False      True   VERB      ROOT                  O\n",
       "6      fancy      fancy    False      True    ADJ      amod                  O\n",
       "7  adventure  adventure    False      True   NOUN  compound                  O\n",
       "8      games       game    False      True   NOUN      dobj                  O"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_nlp(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbf980a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5643b5a7",
   "metadata": {},
   "source": [
    "### Customizing Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "eaec2cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Pete|:|choose|low|-|carb|#|food|#|eat|-|smart|.|_|url|_|;-)|᤮ᝰ\n"
     ]
    }
   ],
   "source": [
    "text = \"@Pete: choose low-carb #food #eat-smart. _url_ ;-) ᤮ᝰ\"\n",
    "doc = nlp(text)\n",
    "\n",
    "print(*doc, sep = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "a9b4288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, \\\n",
    "                        compile_infix_regex, \\\n",
    "                        compile_suffix_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "af81263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(nlp):\n",
    "     # use default patterns except the ones matched by re.search\n",
    "     prefixes = [pattern for pattern in nlp.Defaults.prefixes\n",
    "                     if pattern not in ['-', '_', '#']]\n",
    "     suffixes = [pattern for pattern in nlp.Defaults.suffixes\n",
    "                     if pattern not in ['_']]\n",
    "     infixes = [pattern for pattern in nlp.Defaults.infixes\n",
    "                     if not re.search(pattern, 'xx-xx')]\n",
    "     return Tokenizer(\n",
    "                 vocab = nlp.vocab,\n",
    "                 rules = nlp.Defaults.tokenizer_exceptions,\n",
    "                 prefix_search = compile_prefix_regex(prefixes).search,\n",
    "                 suffix_search = compile_suffix_regex(suffixes).search,\n",
    "                 infix_finditer = compile_infix_regex(infixes).finditer,\n",
    "                 token_match = nlp.Defaults.token_match\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "18494f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "2375848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.tokenizer = custom_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "07d4103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "4f29dd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Pete|:|choose|low-carb|#food|#eat-smart|.|_url_|;-)|᤮ᝰ\n"
     ]
    }
   ],
   "source": [
    "print(*doc, sep = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f786de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "622895ed",
   "metadata": {},
   "source": [
    "### Working with stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "0c458bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dear, Ryan, need, sit, talk, Regards, Pete]\n"
     ]
    }
   ],
   "source": [
    "text = \"Dear Ryan, we need to sit down and talk. Regards, Pete\"\n",
    "doc = nlp(text)\n",
    "non_stop = [t for t in doc if not t.is_stop and not t.is_punct]\n",
    "print(non_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e967c9ce",
   "metadata": {},
   "source": [
    "### Extracting Lemmas Based on Part of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "e368d97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "123dc565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my|good|friend|Ryan|Peters|like|fancy|adventure|game|.\n"
     ]
    }
   ],
   "source": [
    "print(*[t.lemma_ for t in doc], sep = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "e5b8f757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[friend, Ryan, Peters, adventure, games]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc = nlp(text)\n",
    "nouns = [t for t in doc if t.pos_ in ['NOUN', 'PROPN']]\n",
    "print(nouns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "6b963def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "bf155c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = textacy.extract.words(\n",
    "        doc,\n",
    "        filter_nums=True,\n",
    "        filter_punct=True,\n",
    "        filter_stops=True,\n",
    "        include_pos=['ADJ', 'NOUN', 'NUM'],\n",
    "        exclude_pos=None,\n",
    "        min_freq=1 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "808607aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best|friend|fancy|adventure|games\n"
     ]
    }
   ],
   "source": [
    "print(*[t for t in tokens], sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957f6029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "98d79a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lemmas(doc, **kwargs):\n",
    "    return [t.lemma_ for t in textacy.extract.words(doc, **kwargs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "490dae21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good|friend|fancy|adventure|game\n"
     ]
    }
   ],
   "source": [
    "lemmas = extract_lemmas(doc, include_pos=['ADJ', 'NOUN'])\n",
    "print(*lemmas, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e0e2a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9365aa77",
   "metadata": {},
   "source": [
    "### Extracting Noun Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "5dc5ce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "17f86b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\"POS:ADJ POS:NOUN:+\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "7b596282",
   "metadata": {},
   "outputs": [],
   "source": [
    "span = textacy.extract.matches.token_matches(doc, patterns = patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "0e8b7619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good friend|fancy adventure|fancy adventure game\n"
     ]
    }
   ],
   "source": [
    "print(*[s.lemma_ for s in span], sep = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0198bb89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "0164c4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good_friend|fancy_adventure|fancy_adventure_game|adventure_game\n"
     ]
    }
   ],
   "source": [
    "def extract_noun_phrases(doc, preceding_pos=['NOUN'], sep='_'):\n",
    "    patterns = []\n",
    "    for pos in preceding_pos:\n",
    "        patterns.append(f\"POS:{pos} POS:NOUN:+\")\n",
    "    spans = textacy.extract.matches.token_matches(doc, patterns=patterns)\n",
    "    return [sep.join([t.lemma_ for t in s]) for s in spans]\n",
    "\n",
    "print(*extract_noun_phrases(doc, ['ADJ', 'NOUN']), sep = '|')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ec682f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4cc46f8",
   "metadata": {},
   "source": [
    "### : Extracting Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "40b2c6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(James O'Neill, PERSON) (World Cargo Inc, ORG) (San Francisco, GPE) "
     ]
    }
   ],
   "source": [
    "text = \"James O'Neill, chairman of World Cargo Inc, lives in San Francisco.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"({ent.text}, {ent.label_})\", end=\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "78a81429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "4cb89b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    James O'Neill\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", chairman of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    World Cargo Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", lives in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    San Francisco\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6d60ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "fcac9781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(doc, include_types = None, sep = \"_\"):\n",
    "    ents = textacy.extract.entities(\n",
    "            doc,\n",
    "            include_types=include_types,\n",
    "            exclude_types=None,\n",
    "             drop_determiners=True,\n",
    "             min_freq=1)\n",
    "    return [sep.join([t.lemma_ for t in e])+'/'+e.label_ for e in ents]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "22220252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"James_O'Neill/PERSON\", 'San_Francisco/GPE']\n"
     ]
    }
   ],
   "source": [
    "print(extract_entities(doc, ['PERSON', 'GPE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2648ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
